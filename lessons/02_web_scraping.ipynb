{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping con Beautiful Soup.\n",
    "\n",
    "* * * \n",
    "\n",
    "### Iconos utilizados en este cuaderno\n",
    "üîî **Pregunta**: Una pregunta r√°pida para ayudarte a entender lo que est√° pasando.<br>\n",
    "ü•ä **Desaf√≠o**: Ejercicio interactivo. ¬°Trabajaremos en estos durante el taller!<br>\n",
    "‚ö†Ô∏è **Advertencia**: Aviso sobre cosas complicadas o errores comunes.<br>\n",
    "üí° **Consejo**: C√≥mo hacer algo de manera m√°s eficiente o efectiva.<br>\n",
    "üé¨ **Demostraci√≥n**: Mostrar algo m√°s avanzado, para que sepas para qu√© se puede usar Python.<br>\n",
    "\n",
    "### Objetivos de aprendizaje\n",
    "1. [Reflexi√≥n: ¬øRaspado o no raspado?](#when)\n",
    "2. [Extracci√≥n y an√°lisis de HTML](#extract)\n",
    "3. [Raspado de la Asamblea General de Illinois](#scrape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='when'></a>\n",
    "\n",
    "# Scrape o no Scrape\n",
    "\n",
    "Cuando queremos acceder a datos de la web, primero debemos asegurarnos de si el sitio web que nos interesa ofrece una API web. Plataformas como Twitter, Reddit y el New York Times ofrecen APIs. **Consulta el taller de APIs web de [Python de D-Labs](https://github.com/dlab-berkeley/Python-Web-APIs) si quieres aprender a usar APIs.**\n",
    "\n",
    "Sin embargo, a menudo hay casos en los que no existe una API web. En estos casos, podemos recurrir al web scraping, donde extraemos el HTML subyacente de una p√°gina web y obtenemos directamente la informaci√≥n que queremos. Hay varios paquetes en Python que podemos usar para realizar estas tareas. Nos centraremos en dos paquetes: Requests y Beautiful Soup.\n",
    "\n",
    "Nuestro estudio de caso ser√° raspar informaci√≥n sobre los [senadores estatales de Illinois](http://www.ilga.gov/senate), as√≠ como la [lista de proyectos de ley](http://www.ilga.gov/senate/SenatorBills.asp?MemberID=1911&GA=98&Primary=True) que cada senador ha patrocinado. Antes de comenzar, revisa estos sitios web para ver su estructura.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Instalaci√≥n\n",
    "\n",
    "Usaremos dos paquetes principales: [Requests](http://docs.python-requests.org/en/latest/user/quickstart/) y [Beautiful Soup](http://www.crummy.com/software/BeautifulSoup/bs4/doc/). Adelante, instala estos paquetes si a√∫n no lo has hecho:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambi√©n instalaremos el paquete `lxml` que ayuda a soportar parte del an√°lisis que realiza Beautiful Soup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='extract'></a>\n",
    "\n",
    "# Extracci√≥n y An√°lisis de HTML \n",
    "\n",
    "Para realizar correctamente la extracci√≥n y el an√°lisis de HTML, seguiremos los siguientes 4 pasos:\n",
    "1. Realizar una solicitud GET\n",
    "2. Analizar la p√°gina con Beautiful Soup\n",
    "3. Buscar elementos HTML\n",
    "4. Obtener atributos y texto de estos elementos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 1: Realizar una Solicitud GET para Obtener el HTML de una P√°gina\n",
    "\n",
    "Podemos usar la biblioteca Requests para:\n",
    "\n",
    "1. Hacer una solicitud GET a la p√°gina, y  \n",
    "2. Leer el c√≥digo HTML de la p√°gina web.\n",
    "\n",
    "El proceso de realizar una solicitud y obtener un resultado se asemeja al flujo de trabajo de una API web. Sin embargo, en este caso, estamos haciendo una solicitud directamente al sitio web, y tendremos que analizar el HTML por nuestra cuenta. Esto contrasta con recibir datos organizados en un formato m√°s sencillo como `JSON` o `XML`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make a GET request\n",
    "req = requests.get('http://www.ilga.gov/senate/default.asp')\n",
    "# Read the content of the server‚Äôs response\n",
    "src = req.text\n",
    "# View some output\n",
    "print(src[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 2: Analizar la P√°gina con Beautiful Soup\n",
    "\n",
    "Ahora utilizamos la funci√≥n `BeautifulSoup` para analizar la respuesta y convertirla en un √°rbol HTML. Esto devuelve un objeto (llamado **soup object**) que contiene todo el HTML del documento original.\n",
    "\n",
    "Si encuentras un error relacionado con una biblioteca de an√°lisis, aseg√∫rate de haber instalado el paquete `lxml` para proporcionar a Beautiful Soup las herramientas necesarias para el an√°lisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the response into an HTML tree\n",
    "soup = BeautifulSoup(src, 'lxml')\n",
    "# Take a look\n",
    "print(soup.prettify()[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La salida se ve bastante similar a la anterior, pero ahora est√° organizada en un objeto `soup`, lo que nos permite recorrer la p√°gina con mayor facilidad.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 3: Buscar Elementos HTML\n",
    "\n",
    "Beautiful Soup ofrece varias funciones para encontrar componentes √∫tiles en una p√°gina. Permite buscar elementos utilizando:\n",
    "\n",
    "1. Etiquetas HTML  \n",
    "2. Atributos HTML  \n",
    "3. Selectores CSS  \n",
    "\n",
    "Primero, busquemos **etiquetas HTML**.  \n",
    "\n",
    "La funci√≥n `find_all` busca en el √°rbol de `soup` todos los elementos con una etiqueta HTML espec√≠fica y devuelve todos esos elementos.\n",
    "\n",
    "**¬øQu√© hace el siguiente ejemplo?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all elements with a certain tag\n",
    "a_tags = soup.find_all(\"a\")\n",
    "print(a_tags[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que `find_all()` es el m√©todo m√°s popular en la API de b√∫squeda de Beautiful Soup, existe un atajo para usarlo. Si tratas al objeto `BeautifulSoup` como si fuera una funci√≥n, es equivalente a llamar a `find_all()` en ese objeto.\n",
    "\n",
    "Estas dos l√≠neas de c√≥digo son equivalentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a_tags = soup.find_all(\"a\")\n",
    "a_tags_alt = soup(\"a\")\n",
    "print(a_tags[0])\n",
    "print(a_tags_alt[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬øCu√°ntos enlaces obtuvimos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(a_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬°Eso es mucho! Muchos elementos en una p√°gina tendr√°n la misma etiqueta HTML. Por ejemplo, si buscas todo lo que tenga la etiqueta `a`, probablemente obtendr√°s muchos resultados, muchos de los cuales quiz√°s no quieras. Recuerda que la etiqueta `a` define un hiperv√≠nculo, por lo que generalmente encontrar√°s muchos en cualquier p√°gina.\n",
    "\n",
    "¬øQu√© pasa si queremos buscar etiquetas HTML con ciertos atributos, como clases CSS espec√≠ficas?\n",
    "\n",
    "Podemos hacerlo agregando un argumento adicional a la funci√≥n `find_all`. En el ejemplo siguiente, estamos encontrando todas las etiquetas `a` y luego filtrando aquellas con `class_=\"sidemenu\"`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get only the 'a' tags in 'sidemenu' class\n",
    "side_menus = soup(\"a\", class_=\"sidemenu\")\n",
    "side_menus[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una forma m√°s eficiente de buscar elementos en un sitio web es a trav√©s de un **selector CSS**. Para esto, debemos usar un m√©todo diferente llamado `select()`. Solo tienes que pasar una cadena al m√©todo `.select()` para obtener todos los elementos que coincidan con ese selector CSS v√°lido.\n",
    "\n",
    "En el ejemplo anterior, podemos usar `\"a.sidemenu\"` como selector CSS, lo que devuelve todas las etiquetas `a` con la clase `sidemenu`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get elements with \"a.sidemenu\" CSS Selector.\n",
    "selected = soup.select(\"a.sidemenu\")\n",
    "selected[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Desaf√≠o: Encontrar Todos\n",
    "\n",
    "Usa BeautifulSoup para encontrar todos los elementos `a` con la clase `mainmenu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 4: Obtener Atributos y Texto de los Elementos\n",
    "\n",
    "Una vez que identificamos los elementos, queremos acceder a la informaci√≥n dentro de ellos. Generalmente, esto significa dos cosas:\n",
    "\n",
    "1. Texto  \n",
    "2. Atributos  \n",
    "\n",
    "Obtener el texto dentro de un elemento es f√°cil. Todo lo que tenemos que hacer es usar el miembro `text` de un objeto `tag`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get all sidemenu links as a list\n",
    "side_menu_links = soup.select(\"a.sidemenu\")\n",
    "\n",
    "# Examine the first link\n",
    "first_link = side_menu_links[0]\n",
    "print(first_link)\n",
    "\n",
    "# What class is this variable?\n",
    "print('Class: ', type(first_link))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬°Es una etiqueta de Beautiful Soup! Esto significa que tiene un miembro `text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(first_link.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A veces queremos obtener el valor de ciertos atributos. Esto es particularmente relevante para las etiquetas `a`, o enlaces, donde el atributo `href` nos indica a d√≥nde lleva el enlace.\n",
    "\n",
    "üí° **Consejo**: Puedes acceder a los atributos de una etiqueta trat√°ndola como un diccionario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(first_link['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Desaf√≠o: Extraer atributos espec√≠ficos\n",
    "\n",
    "Extrae todos los atributos `href` de cada URL con la clase `mainmenu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='scrape'></a>\n",
    "\n",
    "# Arrastrando la Asamblea General de Illinois\n",
    "Aunque parezca incre√≠ble, estas son las herramientas fundamentales para arrastrar un sitio web. Una vez que dediques m√°s tiempo a familiarizarte con HTML y CSS, simplemente ser√° cuesti√≥n de comprender la estructura de un sitio web en particular y aplicar inteligentemente las herramientas de Beautiful Soup y Python.\n",
    "\n",
    "Apliquemos estas habilidades para arrastrar la [98.¬™ Asamblea General de Illinois](http://www.ilga.gov/senate/default.asp?GA=98)..\n",
    "\n",
    "En concreto, nuestro objetivo es arrastrar informaci√≥n sobre cada senador, incluyendo su nombre, distrito y partido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape and Soup the Webpage\n",
    "\n",
    "Let's scrape and parse the webpage, using the tools we learned in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make a GET request\n",
    "req = requests.get('http://www.ilga.gov/senate/default.asp?GA=98')\n",
    "# Read the content of the server‚Äôs response\n",
    "src = req.text\n",
    "# Soup it\n",
    "soup = BeautifulSoup(src, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for the Table Elements\n",
    "\n",
    "Our goal is to obtain the elements in the table on the webpage. Remember: rows are identified by the `tr` tag. Let's use `find_all` to obtain these elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all table row elements\n",
    "rows = soup.find_all(\"tr\")\n",
    "len(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Warning**: Keep in mind: `find_all` gets *all* the elements with the `tr` tag. We only want some of them. If we use the 'Inspect' function in Google Chrome and look carefully, then we can use some CSS selectors to get just the rows we're interested in. Specifically, we want the inner rows of the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns every ‚Äòtr tr tr‚Äô css selector in the page\n",
    "rows = soup.select('tr tr tr')\n",
    "\n",
    "for row in rows[:5]:\n",
    "    print(row, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we want everything after the first two rows. Let's work with a single row to start, and build our loop from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_row = rows[2]\n",
    "print(example_row.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break this row down into its component cells/columns using the `select` method with CSS selectors. Looking closely at the HTML, there are a couple of ways we could do this.\n",
    "\n",
    "* We could identify the cells by their tag `td`.\n",
    "* We could use the the class name `.detail`.\n",
    "* We could combine both and use the selector `td.detail`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cell in example_row.select('td'):\n",
    "    print(cell)\n",
    "print()\n",
    "\n",
    "for cell in example_row.select('.detail'):\n",
    "    print(cell)\n",
    "print()\n",
    "\n",
    "for cell in example_row.select('td.detail'):\n",
    "    print(cell)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that these are all the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert example_row.select('td') == example_row.select('.detail') == example_row.select('td.detail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the selector `td.detail` to be as specific as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only those 'td' tags with class 'detail' \n",
    "detail_cells = example_row.select('td.detail')\n",
    "detail_cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time, we're interested in the actual **text** of a website, not its tags. Recall that to get the text of an HTML element, we use the `text` member:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the text in each of those cells\n",
    "row_data = [cell.text for cell in detail_cells]\n",
    "\n",
    "print(row_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! Now we just use our basic Python knowledge to get the elements of this list that we want. Remember, we want the senator's name, their district, and their party."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(row_data[0]) # Name\n",
    "print(row_data[3]) # District\n",
    "print(row_data[4]) # Party"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Rid of Junk Rows\n",
    "\n",
    "We saw at the beginning that not all of the rows we got actually correspond to a senator. We'll need to do some cleaning before we can proceed forward. Take a look at some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Row 0:\\n', rows[0], '\\n')\n",
    "print('Row 1:\\n', rows[1], '\\n')\n",
    "print('Last Row:\\n', rows[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we write our for loop, we only want it to apply to the relevant rows. So we'll need to filter out the irrelevant rows. The way to do this is to compare some of these to the rows we do want, see how they differ, and then formulate that in a conditional.\n",
    "\n",
    "As you can imagine, there a lot of possible ways to do this, and it'll depend on the website. We'll show some here to give you an idea of how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad rows\n",
    "print(len(rows[0]))\n",
    "print(len(rows[1]))\n",
    "\n",
    "# Good rows\n",
    "print(len(rows[2]))\n",
    "print(len(rows[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps good rows have a length of 5. Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_rows = [row for row in rows if len(row) == 5]\n",
    "\n",
    "# Let's check some rows\n",
    "print(good_rows[0], '\\n')\n",
    "print(good_rows[-2], '\\n')\n",
    "print(good_rows[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found a footer row in our list that we'd like to avoid. Let's try something else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows[2].select('td.detail') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad row\n",
    "print(rows[-1].select('td.detail'), '\\n')\n",
    "\n",
    "# Good row\n",
    "print(rows[5].select('td.detail'), '\\n')\n",
    "\n",
    "# How about this?\n",
    "good_rows = [row for row in rows if row.select('td.detail')]\n",
    "\n",
    "print(\"Checking rows...\\n\")\n",
    "print(good_rows[0], '\\n')\n",
    "print(good_rows[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we found something that worked!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enlazar todo\n",
    "\n",
    "Ahora que hemos visto c√≥mo obtener los datos que queremos de una fila, as√≠ como filtrar las filas que no queremos, vamos a juntarlo todo en un bucle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define storage list\n",
    "members = []\n",
    "\n",
    "# Get rid of junk rows\n",
    "valid_rows = [row for row in rows if row.select('td.detail')]\n",
    "\n",
    "# Loop through all rows\n",
    "for row in valid_rows:\n",
    "    # Select only those 'td' tags with class 'detail'\n",
    "    detail_cells = row.select('td.detail')\n",
    "    # Keep only the text in each of those cells\n",
    "    row_data = [cell.text for cell in detail_cells]\n",
    "    # Collect information\n",
    "    name = row_data[0]\n",
    "    district = int(row_data[3])\n",
    "    party = row_data[4]\n",
    "    # Store in a tuple\n",
    "    senator = (name, district, party)\n",
    "    # Append to list\n",
    "    members.append(senator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be 61\n",
    "len(members)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Echemos un vistazo a lo que tenemos en  `members`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(members[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä  Desaf√≠o: Obtener elementos `href` que apunten a los proyectos de ley de los miembros\n",
    "\n",
    "El c√≥digo anterior recupera informaci√≥n sobre:\n",
    "- el nombre del senador\n",
    "- su n√∫mero de distrito,\n",
    "- y su partido.\n",
    "\n",
    "Ahora queremos recuperar la URL de la lista de proyectos de ley de cada senador. Cada URL seguir√° un formato espec√≠fico. \n",
    "\n",
    "El formato de la lista de proyectos de ley para un senador determinado es:\n",
    "\n",
    "`http://www.ilga.gov/senate/SenatorBills.asp?GA=98&MemberID=[MEMBER_ID]&Primary=True`\n",
    "\n",
    "para obtener algo como:\n",
    "\n",
    "`http://www.ilga.gov/senate/SenatorBills.asp?MemberID=1911&GA=98&Primary=True`\n",
    "\n",
    "en el que `MEMBER_ID=1911`. \n",
    "\n",
    "Deber√≠a poder ver que, desafortunadamente, `MEMBER_ID` ` no es actualmente algo que se extraiga en nuestro c√≥digo de raspado.\n",
    "\n",
    "Tu tarea inicial es modificar el c√≥digo anterior para que tambi√©n **recuperemos la URL completa que apunta a la p√°gina correspondiente de los proyectos de ley patrocinados por las primarias**, para cada miembro, y la devolvamos junto con su nombre, distrito y partido.\n",
    "\n",
    "Consejos: \n",
    "\n",
    "* Para ello, querr√°s obtener el elemento de anclaje apropiado (`<a>`) en cada fila de legisladores de la tabla. Puedes utilizar de nuevo el m√©todo `.select()` en el objeto `row` en el bucle para hacer esto - similar al comando que encuentra todas las celdas `td.detail` en la fila. Recuerda que s√≥lo queremos el enlace a los proyectos de ley del legislador, no a los comit√©s ni a la p√°gina de perfil del legislador.\n",
    "* El HTML de los elementos de anclaje tendr√° el siguiente aspecto: `<a href=\"/senate/Senator.asp/...\">Bills</a>`. La cadena en el atributo `href` contiene el enlace **relative** que buscamos. Puedes acceder a un atributo de un objeto BeatifulSoup `Tag` de la misma forma que accedes a un diccionario de Python: `anchor['attributeName']`. V√©ase el <a href=\"http://www.crummy.com/software/BeautifulSoup/bs4/doc/#tag\">documentation</a> para m√°s detalles.\n",
    "* Hay un mont√≥n de maneras diferentes de utilizar BeautifulSoup para hacer las cosas. lo que usted necesita hacer para sacar el `href` est√° bien.\n",
    "\n",
    "El c√≥digo ha sido parcialmente rellenado para usted. Rell√©nalo donde dice `#YOUR CODE HERE`. Guarda la ruta en un objeto llamado `full_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make a GET request\n",
    "req = requests.get('http://www.ilga.gov/senate/default.asp?GA=98')\n",
    "# Read the content of the server‚Äôs response\n",
    "src = req.text\n",
    "# Soup it\n",
    "soup = BeautifulSoup(src, \"lxml\")\n",
    "# Create empty list to store our data\n",
    "members = []\n",
    "\n",
    "# Returns every ‚Äòtr tr tr‚Äô css selector in the page\n",
    "rows = soup.select('tr tr tr')\n",
    "# Get rid of junk rows\n",
    "rows = [row for row in rows if row.select('td.detail')]\n",
    "\n",
    "# Loop through all rows\n",
    "for row in rows:\n",
    "    # Select only those 'td' tags with class 'detail'\n",
    "    detail_cells = row.select('td.detail') \n",
    "    # Keep only the text in each of those cells\n",
    "    row_data = [cell.text for cell in detail_cells]\n",
    "    # Collect information\n",
    "    name = row_data[0]\n",
    "    district = int(row_data[3])\n",
    "    party = row_data[4]\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    full_path = ''\n",
    "\n",
    "    # Store in a tuple\n",
    "    senator = (name, district, party, full_path)\n",
    "    # Append to list\n",
    "    members.append(senator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment to test \n",
    "# members[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä  Desaf√≠o: Modularice su c√≥digo\n",
    "\n",
    "Convierta el c√≥digo anterior en una funci√≥n que acepte una URL, rastree la URL en busca de sus senadores y devuelva una lista de tuplas con informaci√≥n sobre cada senador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def get_members(url):\n",
    "    return [___]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test your code\n",
    "url = 'http://www.ilga.gov/senate/default.asp?GA=98'\n",
    "senate_members = get_members(url)\n",
    "len(senate_members)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Reto para llevar a casa: Escribir una funci√≥n rascadora\n",
    "\n",
    "Queremos raspar las p√°ginas web correspondientes a los proyectos de ley patrocinados por cada uno de ellos.\n",
    "Escribe una funci√≥n llamada `get_bills(url)` para analizar una URL de facturas dada. Esto implicar√°:\n",
    "\n",
    "  - solicitando la URL mediante la biblioteca <a href=\"http://docs.python-requests.org/en/latest/\">`requests`</a>   - utilizando las caracter√≠sticas de la biblioteca `BeautifulSoup` para encontrar todos los elementos `<td>` con la clase `billlist`.\n",
    "  - devuelve una _lista_ de tuplas, cada una con:\n",
    "      - descripci√≥n (2¬™ columna)\n",
    "      - c√°mara (S o H) (3¬™ columna)\n",
    "      - la √∫ltima acci√≥n (4¬™ columna)\n",
    "      - la fecha de la √∫ltima acci√≥n (5¬™ columna)\n",
    "      \n",
    "Esta funci√≥n se ha completado parcialmente. Rellene el resto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bills(url):\n",
    "    src = requests.get(url).text\n",
    "    soup = BeautifulSoup(src)\n",
    "    rows = soup.select('tr')\n",
    "    bills = []\n",
    "    for row in rows:\n",
    "        # YOUR CODE HERE\n",
    "        bill_id =\n",
    "        description =\n",
    "        chamber =\n",
    "        last_action =\n",
    "        last_action_date =\n",
    "        bill = (bill_id, description, chamber, last_action, last_action_date)\n",
    "        bills.append(bill)\n",
    "    return bills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment to test your code\n",
    "# test_url = senate_members[0][3]\n",
    "# get_bills(test_url)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rechazar todas las facturas\n",
    "\n",
    "Por √∫ltimo, crea un diccionario `bills_dict` que asigne un n√∫mero de distrito (la clave) a una lista de proyectos de ley (el valor) procedentes de ese distrito. Puedes hacerlo recorriendo todos los miembros del senado en `members_dict` y llamando a `get_bills()` para cada una de sus URLs de proyectos de ley asociadas.\n",
    "\n",
    "**NOTA:** por favor llama a la funci√≥n `time.sleep(1)` en cada iteraci√≥n del bucle, para que no destruyamos la web del estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment to test your code\n",
    "# bills_dict[52]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b6f9fe9f4b7182690503d8ecc2bae97b0ee3ebf54e877167ae4d28c119a56988"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
