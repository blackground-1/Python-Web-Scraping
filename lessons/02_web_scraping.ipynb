{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping con Beautiful Soup\n",
    "\n",
    "* * * \n",
    "\n",
    "### Iconos utilizados en este cuaderno\n",
    "üîî **Pregunta**: Una pregunta r√°pida para ayudarte a entender lo que est√° pasando.<br>\n",
    "ü•ä **Desaf√≠o**: Ejercicio interactivo. ¬°Trabajaremos en estos durante el taller!<br>\n",
    "‚ö†Ô∏è **Advertencia**: Aviso sobre cosas complicadas o errores comunes.<br>\n",
    "üí° **Consejo**: C√≥mo hacer algo de manera m√°s eficiente o efectiva.<br>\n",
    "üé¨ **Demostraci√≥n**: Mostrar algo m√°s avanzado, para que sepas para qu√© se puede usar Python.<br>\n",
    "\n",
    "### Objetivos de aprendizaje\n",
    "1. [Reflexi√≥n: ¬øRaspado o no raspado?](#when)\n",
    "2. [Extracci√≥n y an√°lisis de HTML](#extract)\n",
    "3. [Raspado de la Asamblea General de Illinois](#scrape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='when'></a>\n",
    "\n",
    "# Scrape o no Scrape\n",
    "\n",
    "Cuando queremos acceder a datos de la web, primero debemos asegurarnos de si el sitio web que nos interesa ofrece una API web. Plataformas como Twitter, Reddit y el New York Times ofrecen APIs. **Consulta el taller de APIs web de [Python de D-Labs](https://github.com/dlab-berkeley/Python-Web-APIs) si quieres aprender a usar APIs.**\n",
    "\n",
    "Sin embargo, a menudo hay casos en los que no existe una API web. En estos casos, podemos recurrir al web scraping, donde extraemos el HTML subyacente de una p√°gina web y obtenemos directamente la informaci√≥n que queremos. Hay varios paquetes en Python que podemos usar para realizar estas tareas. Nos centraremos en dos paquetes: Requests y Beautiful Soup.\n",
    "\n",
    "Nuestro estudio de caso ser√° raspar informaci√≥n sobre los [senadores estatales de Illinois](http://www.ilga.gov/senate), as√≠ como la [lista de proyectos de ley](http://www.ilga.gov/senate/SenatorBills.asp?MemberID=1911&GA=98&Primary=True) que cada senador ha patrocinado. Antes de comenzar, revisa estos sitios web para ver su estructura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalaci√≥n\n",
    "\n",
    "Usaremos dos paquetes principales: [Requests](http://docs.python-requests.org/en/latest/user/quickstart/) y [Beautiful Soup](http://www.crummy.com/software/BeautifulSoup/bs4/doc/). Adelante, instala estos paquetes si a√∫n no lo has hecho:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambi√©n instalaremos el paquete `lxml` que ayuda a soportar parte del an√°lisis que realiza Beautiful Soup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import time"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b6f9fe9f4b7182690503d8ecc2bae97b0ee3ebf54e877167ae4d28c119a56988"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
